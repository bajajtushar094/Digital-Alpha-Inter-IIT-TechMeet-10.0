# -*- coding: utf-8 -*-
"""itemwisesummary.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wGH9AFpt1ndGl9mT-DoPH4tpQCoqkG-b
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip -q install -q pandas nltk transformers
import nltk
import pandas as pd
from transformers import pipeline
# nltk.download('all')
!pip install huggingface

from google.colab import drive
drive.mount('/content/drive')

df10k = pd.read_csv('/content/drive/MyDrive/10k extracted/extracted_data_10K.csv')
print(df10k.shape)
# print(df10k.head())

# summarizer = pipeline('summarization',model='gpt2')

# classifier_model_name = 'bhadresh-savani/distilbert-base-uncased-emotion'
# # classifier_model_name = 'nickmuchi/sec-bert-finetuned-finance-classification'
# classifier_emotions = ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise','love']

# #classifier_model_name = 'ProsusA I/finbert'
# #classifier_emotions = ['positive', 'neutral', 'negative']

# classifier = pipeline('text-classification', model=classifier_model_name)

!nvidia-smi

# def find_emotional_sentences(text, emotions, threshold):
#     sentences_by_emotion = {}
#     for e in emotions:
#         sentences_by_emotion[e]=[]
#     sentences = nltk.sent_tokenize(text)
#     print(f'Document has {len(text)} characters and {len(sentences)} sentences.')
#     for s in sentences:
#         prediction = classifier(s)
#         if (prediction[0]['label']!='neutral' and prediction[0]['score']>threshold):
#             #print (f'Sentence #{sentences.index(s)}: {prediction} {s}')
#             sentences_by_emotion[prediction[0]['label']].append(s)
#     for e in emotions:
#         print(f'{e}: {len(sentences_by_emotion[e])} sentences')
#     return sentences_by_emotion

# def summarize_sentences(sentences_by_emotion, min_length,max_length):
#     for k in sentences_by_emotion.keys():
#         if (len(sentences_by_emotion[k])!=0):
#             text = ' '.join(sentences_by_emotion[k])
#             summary = summarizer(text, min_length=min_length,max_length=max_length)
#             print(f"{k.upper()}: {summary[0]['summary_text']}\n")

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

tokenizer = AutoTokenizer.from_pretrained("facebook/bart-large-cnn")

model = AutoModelForSeq2SeqLM.from_pretrained("facebook/bart-large-cnn")

summarizer = pipeline("summarization", model=model, tokenizer=tokenizer)

# GPT2_model = TransformerSummarizer(transformer_type="Bert",transformer_model_key="bert_base-uncased",gpu_id=0)

import re

text=df10k['item_1'].iloc[0]
text = re.sub(r'[^a-zA-Z0-9 .,]', '', text)
text = ' '.join(text.split())
text

len(x[0]['summary_text'])

x

df10k.dtypes

arr_temp = []
for i in range(len(df10k)):
  for j in df10k.columns:
    if j!='filename':
      text = str(df10k[j].iloc[i])
      # text=df10k['item_1'].iloc[0]
      text = re.sub(r'[^a-zA-Z0-9 .,]', '', text)
      text = ' '.join(text.split())
      # sentences_by_emotion = find_emotional_sentences(text, classifier_emotions, 0.7)
      temp = summarizer(text,truncation=True,max_length=1024,min_length=10)
      # print(temp)
      print(len(temp[0]['summary_text']))
      df10k[f"{j}_summary"]=temp[0]['summary_text']
      print('-'*50)
      arr_temp.append(temp[0]['summary_text'])
  print(" ")
  print("*"*100)
  print(f"Completed for Row {i}")
  print("*"*100)
  print(" ")

# print(len(df10k['item_1'][1]))
# print(len(df10k['item_1_summary'][1]))
# print("---------------------------------Original-------------------------")
# print(df10k['item_1'][1])
# print("---------------------------------Summary-------------------------")
# print(df10k['item_1_summary'][1])

df10k.to_csv("itemwise10ksummary.csv",index=False)

!pip install bert-extractive-summarizer
!pip install transformers
!pip install spacy

from summarizer import Summarizer,TransformerSummarizer

body = '''
The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.'''

GPT2_model = TransformerSummarizer(transformer_type="GPT2",transformer_model_key="gpt2-medium",gpu_id=0)
# full = ''.join(GPT2_model(body, min_length=60))
# print(full)

x=GPT2_model(df10k['item_1'][1], min_length=50)

len(df10k['item_1'][1])

len(x)

import torch
torch.cuda.is_available()

from transformers import PegasusForConditionalGeneration, PegasusTokenizer
model_name = 'tuner007/pegasus_paraphrase'
torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'

tokenizer = PegasusTokenizer.from_pretrained(model_name)
model = PegasusForConditionalGeneration.from_pretrained(model_name).to(torch_device)

df10k['item_7'][8]

pip install rank-bm25

