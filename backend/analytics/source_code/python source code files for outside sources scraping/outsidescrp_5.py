# -*- coding: utf-8 -*-
"""buttonclickscrape_quarter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kobT8RVhNp-je56rbl-0IIEAEm6iGP9N
"""

import pandas as pd
from bs4 import BeautifulSoup
import re
from selenium import webdriver
import chromedriver_binary
import string

pd.options.display.float_format = '{:.0f}'.format

import lxml
import time

#function to make all values numerical (or - for NaNs)
def convert_to_numeric(column):

    first_col = [i.replace(',','') for i in column]
    second_col = [i.replace('-','') for i in first_col]
    final_col = pd.to_numeric(second_col)
    
    return final_col

ticks = pd.read_csv('cik_with_ticker.csv')

ticker_list = list(ticks['ticker_x'])
cik_list = list(ticks['CIK'])

len(ticker_list)

"""not extracted - 
FICO
"""

# ticker_list_2 = ticker_list[142:]
# ticker_list_3 = ticker_list_2
# ticker_list_3.reverse()
# ticker_list_3 = ticker_list_3[3:]
# print(ticker_list_3)

# tickers_list = ['AAPL','FB']
gctr = 0 
errs = []

for ticker in ticker_list:
    cik = cik_list[gctr]
    gctr = gctr + 1
    is_link = 'https://finance.yahoo.com/quote/'+str(ticker)+'/financials?p='+str(ticker)
    bs_link = 'https://finance.yahoo.com/quote/'+str(ticker)+'/balance-sheet?p='+str(ticker)
    cf_link = 'https://finance.yahoo.com/quote/'+str(ticker)+'/cash-flow?p='+str(ticker)    
    path = r"C:/Users/windows 10/Downloads/chromedriver_win32/chromedriver.exe"    
    driver = webdriver.Chrome(path)
    ctr = 0


    for link in [is_link,bs_link,cf_link]:
        try:
            driver.get(link)
            time.sleep(1)
            button1 = driver.find_element_by_xpath('//*[@id="Col1-1-Financials-Proxy"]/section/div[1]/div[2]/button/div/span') # quarterly
            button1.click()
            time.sleep(1)
            button2 = driver.find_element_by_xpath('//*[@id="Col1-1-Financials-Proxy"]/section/div[2]/button/div/span') # expand
            button2.click()
            time.sleep(1)
            html = driver.execute_script('return document.body.innerHTML;')
            soup = BeautifulSoup(html,'lxml')
            driver.quit()
            features = soup.find_all('div', class_='D(tbr)')

            headers = []
            temp_list = []
            label_list = []
            final = []
            index = 0

            #create headers
            for item in features[0].find_all('div', class_='D(ib)'):
                headers.append(item.text)

            #statement contents
            while index <= len(features)-1:
                #filter for each line of the statement
                temp = features[index].find_all('div', class_='D(tbc)')
                for line in temp:
                    #each item adding to a temporary list
                    temp_list.append(line.text)
                #temp_list added to final list
                final.append(temp_list)
                #clear temp_list
                temp_list = []
                index+=1

            df = pd.DataFrame(final[1:])
            df.columns = headers

            for column in headers[1:]:
        
                df[column] = convert_to_numeric(df[column])

            strs = ['is','bs','cf']

            final_df = df.fillna('-')
            filenam = 'quarter_button/'+str(cik)+'_'+str(ctr)+".csv"
            ctr+=1
            final_df.to_csv(filenam)

        except:
            errs.append(link)

        finally:
            continue


print(errs)

errs

len(errs)

